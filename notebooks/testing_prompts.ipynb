{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the config.yaml file and environment variables\n",
    "ROOT_PATH = os.getenv(\"ROOT_PATH\")\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "with open( os.path.join(ROOT_PATH,'config.yaml'), 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API key and model\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_LLM_MODEL = config['models']['OPENAI_LLM_MODEL']\n",
    "JARVIS_ENDPOINT = \"https://api.jarvislabs.net/openai/72513de9b2d0e6c5/v1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Code Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rag.retrieve_data import RAGRetriever\n",
    "retriever = RAGRetriever()\n",
    "game_id = 1  # Example game ID\n",
    "code_prompt = retriever.get_metadata_prompt(game_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Code_Prompt with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OPENAI_LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code_openai(prompt: str):\n",
    "    \"\"\"Generates code using OpenAI's GPT model.\"\"\"\n",
    "    try:\n",
    "        # Call the OpenAI Completion API\n",
    "        print(\"Generating code using OpenAI's Model: \", OPENAI_LLM_MODEL)      \n",
    "        completion = client.chat.completions.create(\n",
    "            model=OPENAI_LLM_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": code_prompt}\n",
    "            ]\n",
    "            )\n",
    "        generated_code = completion.choices[0].message.content\n",
    "        return generated_code\n",
    "    except Exception as e:\n",
    "        return f\"OpenAI API error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = generate_code_openai(code_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_code = completion.choices[0].message.content\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Code_Prompt with Jarvis Ollama Endpoint - codellama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the config.yaml file and environment variables\n",
    "ROOT_PATH = os.getenv(\"ROOT_PATH\")\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "with open( os.path.join(ROOT_PATH,'config.yaml'), 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a Python code for game TicTac to be played in UI. The game should have a 3x3 grid and two players. The players should be able to take turns to place their mark on the grid. The game should end when one of the players has three marks in a row, column, or diagonal. The game should display the winner or a draw if there is no winner.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JARVIS_OLLAMA_CODE_ENDPOINT = os.getenv(\"JARVIS_OLLAMA_CODE_ENDPOINT\")\n",
    "JARVIS_API_KEY = os.getenv(\"JARVIS_API_KEY\")\n",
    "print(JARVIS_OLLAMA_CODE_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JarvisLabs API details\n",
    "jarvis_api_key = os.getenv(\"JARVIS_API_KEY\")\n",
    "jarvis_code_endpoint = os.getenv(\"JARVIS_OLLAMA_CODE_ENDPOINT\")\n",
    "\n",
    "\n",
    "# JarvisLabs - Ollama Models\n",
    "jarvis_code_llm = config['models']['JARVIS_CODE_LLM']\n",
    "jarvis_rules_llm = config['models']['JARVIS_RULES_LLM']\n",
    "jarvis_client = OpenAI(\n",
    "            base_url=jarvis_code_endpoint,\n",
    "            api_key=jarvis_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def is_jarvis_available():\n",
    "        \"\"\"\n",
    "        Check if the JarvisLabs API is available by making a simple request.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if JarvisLabs is available, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            client = OpenAI(base_url=JARVIS_OLLAMA_CODE_ENDPOINT,api_key=JARVIS_API_KEY)\n",
    "            completion = client.chat.completions.create(model=jarvis_rules_llm,\n",
    "                                                        messages=[{\"role\": \"user\", \"content\": \"Hi\"}])\n",
    "            \n",
    "            if completion:\n",
    "                print(f\"Response from JarvisLabs: {completion.choices[0].message.content}\")\n",
    "                print(\"JarvisLabs is available.\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"[JARVIS][UNAVAILABLE]: JarvisLabs returned status code: {response.status_code}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"JarvisLabs is unavailable: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_jarvis_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "JARVIS_OLLAMA_CODE_ENDPOINT = os.getenv(\"JARVIS_OLLAMA_CODE_ENDPOINT\")\n",
    "JARVIS_API_KEY = os.getenv(\"JARVIS_API_KEY\")\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=JARVIS_OLLAMA_CODE_ENDPOINT,\n",
    "    api_key=JARVIS_API_KEY,\n",
    ")\n",
    "print(JARVIS_OLLAMA_CODE_ENDPOINT)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"codellama\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
