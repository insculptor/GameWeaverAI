{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# # Add the Application root to Python path\n",
    "# ROOT_PATH = os.path.abspath(os.path.join( os.getcwd(), '..'))\n",
    "# sys.path.insert(0, ROOT_PATH)\n",
    "# print(f\"Added {ROOT_PATH} to the Python path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the config.yaml file and environment variables\n",
    "ROOT_PATH = os.path.abspath(os.path.join( os.getcwd(), '..'))\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "with open( os.path.join(ROOT_PATH,'config.yaml'), 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\erdrr\\\\OneDrive\\\\Desktop\\\\KB\\\\Projects\\\\GameWeaverAI'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_PATH = os.getenv(\"ROOT_PATH\")\n",
    "ROOT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API key and model\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_LLM_MODEL = config['models']['OPENAI_LLM_MODEL']\n",
    "JARVIS_ENDPOINT = \"https://api.jarvislabs.net/openai/72513de9b2d0e6c5/v1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo-instruct\"\n",
    "def generate_code_openai(prompt: str):\n",
    "    \"\"\"Generates code using OpenAI's GPT model.\"\"\"\n",
    "    try:\n",
    "        # Call the OpenAI Completion API\n",
    "        response = client.chat.completions.with_raw_response.create(\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }],\n",
    "            model=model,\n",
    "        )\n",
    "        print(response.headers.get('x-request-id'))\n",
    "\n",
    "\n",
    "        generated_code = response.parse()\n",
    "        generated_code = response#.choices[0].text.strip()\n",
    "        print(generated_code)\n",
    "        return generated_code\n",
    "    except Exception as e:\n",
    "        return f\"OpenAI API error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Generate Python code for a Tic-Tac-Toe game.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_game = generate_code_openai(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OpenAI API error: Error code: 404 - {'error': {'message': 'This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erdrr\\OneDrive\\Desktop\\KB\\Projects\\GameWeaverAI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_PATH: c:\\Users\\erdrr\\OneDrive\\Desktop\\KB\\Projects\\GameWeaverAI\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\erdrr\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Connected to Huggingface\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 41 files: 100%|██████████| 41/41 [00:00<00:00, 1343.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'dunzhang/stella_en_1.5B_v5' is available at C:\\Users\\erdrr\\OneDrive\\Desktop\\KB\\Projects\\gameweaverai\\models\\dunzhang\\stella_en_1.5B_v5\n",
      "Model and tokenizer for 'dunzhang/stella_en_1.5B_v5' initialized successfully.\n",
      "Generated Code Prompt:\n",
      " \n",
      "    You are a Python expert and you are tasked with generating the code for a game. Here are the components of the game:\n",
      "\n",
      "    Overview: None\n",
      "    Game Setup: None\n",
      "    How to Play: None\n",
      "    Winning the Game: None\n",
      "    Game Strategy: None\n",
      "    End of Game: None\n",
      "\n",
      "    Based on the above information, generate the Python code for this game. Make sure the game is functional and can be played in a terminal or web interface.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from src.rag.retrieve_data import RAGRetriever\n",
    "\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = RAGRetriever()\n",
    "\n",
    "# Fetch and prepare the code generation prompt for a specific game\n",
    "game_id = 1  # Example game ID\n",
    "prompt = retriever.get_metadata_prompt(game_id)\n",
    "\n",
    "if prompt:\n",
    "    print(\"Generated Code Prompt:\\n\", prompt)\n",
    "else:\n",
    "    print(f\"No metadata found for game ID: {game_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "req_ef74f62c60a52745f44771580ba73712\n",
      "ChatCompletion(id='chatcmpl-A4bAUvRSSWUaodvRhPdyAkTTumdh7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='This is a test. How can I assist you further?', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1725659550, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_483d39d857', usage=CompletionUsage(completion_tokens=12, prompt_tokens=12, total_tokens=24))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = client.chat.completions.with_raw_response.create(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Say this is a test\",\n",
    "    }],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "print(response.headers.get('x-request-id'))\n",
    "\n",
    "# get the object that `chat.completions.create()` would have returned\n",
    "completion = response.parse()\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Jarvis Ollama Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a Python function that takes a list of integers and returns the sum of all the integers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "JARVIS_OLLAMA_CODE_ENDPOINT = os.getenv(\"JARVIS_OLLAMA_CODE_ENDPOINT\")\n",
    "JARVIS_API_KEY = os.getenv(\"JARVIS_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PYTHON]\n",
      "def get_sum(my_list):\n",
      "    return sum(my_list)\n",
      "[/PYTHON]\n",
      "[TESTS]\n",
      "# Test case 1:\n",
      "assert get_sum([]) == 0\n",
      "# Test case 2:\n",
      "assert get_sum([1]) == 1\n",
      "# Test case 3:\n",
      "assert get_sum([1, 2, 3, 4, 5]) == 15\n",
      "[/TESTS]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "JARVIS_OLLAMA_CODE_ENDPOINT = os.getenv(\"JARVIS_OLLAMA_CODE_ENDPOINT\")\n",
    "JARVIS_API_KEY = os.getenv(\"JARVIS_API_KEY\")\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=JARVIS_OLLAMA_CODE_ENDPOINT,\n",
    "    api_key=JARVIS_API_KEY,\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"codellama\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
